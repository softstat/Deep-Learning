{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-WQ_tQxCrEF"
      },
      "source": [
        "이 노트북은 [케라스 창시자에게 배우는 딥러닝 2판](https://tensorflow.blog/kerasdl2/)의 예제 코드를 담고 있습니다.\n",
        "\n",
        "<table align=\"left\">\n",
        "    <tr>\n",
        "        <td>\n",
        "            <a href=\"https://colab.research.google.com/github/rickiepark/deep-learning-with-python-2nd/blob/main/chapter12_part01_text-generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "        </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THz7hp89CrEI"
      },
      "source": [
        "# 생성 모델을 위한 딥러닝"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBnsAPKECrEI"
      },
      "source": [
        "## 텍스트 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D89JwqPBCrEJ"
      },
      "source": [
        "### 시퀀스 생성을 위한 딥러닝 모델의 간단한 역사"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4P5rIr6CrEJ"
      },
      "source": [
        "### 시퀀스 데이터를 어떻게 생성할까?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBs8uChTCrEJ"
      },
      "source": [
        "### 샘플링 전략의 중요성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMcqO4_VCrEK"
      },
      "source": [
        "**다른 온도 값을 사용하여 확률 분포의 가중치 바꾸기**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "baD0AHOUCrEK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def reweight_distribution(original_distribution, temperature=0.5):\n",
        "    distribution = np.log(original_distribution) / temperature\n",
        "    distribution = np.exp(distribution)\n",
        "    return distribution / np.sum(distribution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hCKGeqJCrEL"
      },
      "source": [
        "### 케라스를 사용한 텍스트 생성 모델 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmTMyS1ZCrEM"
      },
      "source": [
        "#### 데이터 준비"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IM6Umf4uCrEM"
      },
      "source": [
        "**IMDB 영화 리뷰 데이터셋 다운로드하고 압축 풀기**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fS_z2t7PCrEN",
        "outputId": "137f7175-e16c-4c22-a3b5-b8ae199404a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-28 01:52:07--  https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  6.02MB/s    in 23s     \n",
            "\n",
            "2024-05-28 01:52:31 (3.44 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcHe7T_QCrEN"
      },
      "source": [
        "**텍스트 파일(한 파일 = 한 샘플)에서 데이터셋 만들기**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hb2YjZ86CrEO",
        "outputId": "90959219-77ef-46aa-f715-ec8dfb211cb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 100006 files belonging to 1 classes.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "dataset = keras.utils.text_dataset_from_directory(\n",
        "    directory=\"aclImdb\", label_mode=None, batch_size=256)\n",
        "dataset = dataset.map(lambda x: tf.strings.regex_replace(x, \"<br />\", \" \"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-6pyZNQCrEO"
      },
      "source": [
        "**`TextVectorization` 층 준비하기**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aHTeHqNfCrEO"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "sequence_length = 100\n",
        "vocab_size = 15000\n",
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "text_vectorization.adapt(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLhj695ZCrEO"
      },
      "source": [
        "**언어 모델링 데이터셋 만들기**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pCPsdys6CrEP"
      },
      "outputs": [],
      "source": [
        "def prepare_lm_dataset(text_batch):\n",
        "    vectorized_sequences = text_vectorization(text_batch)\n",
        "    x = vectorized_sequences[:, :-1]\n",
        "    y = vectorized_sequences[:, 1:]\n",
        "    return x, y\n",
        "\n",
        "lm_dataset = dataset.map(prepare_lm_dataset, num_parallel_calls=4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: IM_dataset에서 첫번째 샘플 출력해줘\n",
        "\n",
        "for x, y in lm_dataset.take(1):\n",
        "  print(x[0])\n",
        "  print(y[0])\n"
      ],
      "metadata": {
        "id": "YcU5nsmg5_ED",
        "outputId": "ca9e2e44-9445-4ef8-b44e-59f9ada0810e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[  252 12486     4   969     5     1  4138    10   212  5489     9     7\n",
            "    22    14   155    14    10  1836   195     9     7   123  1348  1505\n",
            "  1851   124   530    45  1111 13999   110     1  4061    30  2193 12574\n",
            "     3  8011  5215   735   117     2  3056  2165    36  1166     1     7\n",
            "     8  1218 13213    29    31     1    11     3     1     1     1  1865\n",
            "     1  4138  2897    22    12   224    77 11235  6717     1  3472  1424\n",
            "   104     9   172     2  4075   346     5   145   876    15     1  1736\n",
            "    13   521     3     2  1403   140    26  2370     5   717   170     4\n",
            "   935   215    20], shape=(99,), dtype=int64)\n",
            "tf.Tensor(\n",
            "[12486     4   969     5     1  4138    10   212  5489     9     7    22\n",
            "    14   155    14    10  1836   195     9     7   123  1348  1505  1851\n",
            "   124   530    45  1111 13999   110     1  4061    30  2193 12574     3\n",
            "  8011  5215   735   117     2  3056  2165    36  1166     1     7     8\n",
            "  1218 13213    29    31     1    11     3     1     1     1  1865     1\n",
            "  4138  2897    22    12   224    77 11235  6717     1  3472  1424   104\n",
            "     9   172     2  4075   346     5   145   876    15     1  1736    13\n",
            "   521     3     2  1403   140    26  2370     5   717   170     4   935\n",
            "   215    20 12212], shape=(99,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZDJxIp9CrEP"
      },
      "source": [
        "#### 트랜스포머 기반의 시퀀스-투-시퀀스 모델"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "O40uNWAWCrEP"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "          num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "          num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(TransformerDecoder, self).get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    # def get_causal_attention_mask(self, inputs):\n",
        "    #     input_shape = tf.shape(inputs)\n",
        "    #     batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "    #     i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "    #     j = tf.range(sequence_length)\n",
        "    #     mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "    #     mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "    #     mult = tf.concat(\n",
        "    #         [tf.expand_dims(batch_size, -1),\n",
        "    #          tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "    #     return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        # causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        # if mask is not None:\n",
        "        #     padding_mask = tf.cast(\n",
        "        #         mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "        #     padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            use_causal_mask=True)\n",
        "            # attention_mask=causal_mask)\n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs\n",
        "            # attention_mask=padding_mask,\n",
        "        )\n",
        "        attention_output_2 = self.layernorm_2(\n",
        "            attention_output_1 + attention_output_2)\n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "        return self.layernorm_3(attention_output_2 + proj_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qO94YIGCrEQ"
      },
      "source": [
        "**간단한 트랜스포머 기반 언어 모델**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LGCHdjUqCrEQ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "embed_dim = 256\n",
        "latent_dim = 2048\n",
        "num_heads = 2\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, x)\n",
        "outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXrXEm3ICrEQ"
      },
      "source": [
        "### 가변 온도 샘플링을 사용한 텍스트 생성 콜백"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeARJmFICrEQ"
      },
      "source": [
        "**텍스트 생성 콜백**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RtzXqzI1CrER"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "tokens_index = dict(enumerate(text_vectorization.get_vocabulary()))\n",
        "\n",
        "def sample_next(predictions, temperature=1.0):\n",
        "    predictions = np.asarray(predictions).astype(\"float64\")\n",
        "    predictions = np.log(predictions) / temperature\n",
        "    exp_preds = np.exp(predictions)\n",
        "    predictions = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, predictions, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "class TextGenerator(keras.callbacks.Callback):\n",
        "    def __init__(self,\n",
        "                 prompt,\n",
        "                 generate_length,\n",
        "                 model_input_length,\n",
        "                 temperatures=(1.,),\n",
        "                 print_freq=1):\n",
        "        self.prompt = prompt\n",
        "        self.generate_length = generate_length\n",
        "        self.model_input_length = model_input_length\n",
        "        self.temperatures = temperatures\n",
        "        self.print_freq = print_freq\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if (epoch + 1) % self.print_freq != 0:\n",
        "            return\n",
        "        for temperature in self.temperatures:\n",
        "            print(\"== Generating with temperature\", temperature)\n",
        "            sentence = self.prompt\n",
        "            for i in range(self.generate_length):\n",
        "                tokenized_sentence = text_vectorization([sentence])\n",
        "                predictions = self.model(tokenized_sentence)\n",
        "                next_token = sample_next(predictions[0, i, :], temperature)\n",
        "                sampled_token = tokens_index[next_token]\n",
        "                sentence += \" \" + sampled_token\n",
        "            print(sentence)\n",
        "\n",
        "prompt = \"This movie\"\n",
        "text_gen_callback = TextGenerator(\n",
        "    prompt,\n",
        "    generate_length=50,\n",
        "    model_input_length=sequence_length,\n",
        "    temperatures=(0.2, 0.5, 0.7, 1., 1.5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZIt8p-oCrER"
      },
      "source": [
        "**언어 모델 훈련하기**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PeBx-DacCrER",
        "outputId": "279c6d25-62f2-4e58-b277-0bc5c5f2fa80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 5.9231== Generating with temperature 0.2\n",
            "This movie movie was was a a lot lot of of the the movie first was time a i few was years a ago lot and of i the was [UNK] the and film i was was the a movie lot was of the the first best part part of of the\n",
            "== Generating with temperature 0.5\n",
            "This movie movie was was a lucky good so movie i was was not popular a i [UNK] was [UNK] so and horrible i acting was and a i man was who a was lot made of for the the first way time that i i liked have the seen first it\n",
            "== Generating with temperature 0.7\n",
            "This movie film was is awful one movies of like the about storyline it but was it a wasnt waste in of much the better [UNK] than convoluted a [UNK] few movies minutes presented of the the movie spells is of a [UNK] charming and and [UNK] the in role the in\n",
            "== Generating with temperature 1.0\n",
            "This movie movie happened and being [UNK] michael the alot smaller a oscar collection and acclaimed discover was this decided is to kind people of who them bobby so turkey glad up they the this island group spectrum of of the time avery was the laughable segment [UNK] are is douglas none\n",
            "== Generating with temperature 1.5\n",
            "This movie 3 1 rugged did masses mostly for roger milieu ditto folks the morgan nearby in the friday daughter lees finds harvest john ended singer ties where imagines father furniture ahole peter falls valentino mothers some defeats cliched cast at needs all to refuses kidding whether yourself bad stare allows rendition\n",
            "391/391 [==============================] - 167s 415ms/step - loss: 5.9231\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 5.4529== Generating with temperature 0.2\n",
            "This movie is was a a very very good good movie movie i i was was very a good good movie and was the a acting lot was of very the good characters and were the so acting bad was and a the good acting and was the really [UNK] bad and\n",
            "== Generating with temperature 0.5\n",
            "This movie is is the a worst great film movie it it is is a not great a movie great that movie she is doesnt a make good a as very a good lot but of the the camera [UNK] work the and film the the [UNK] story is is not a\n",
            "== Generating with temperature 0.7\n",
            "This movie is is a the must [UNK] have [UNK] to [UNK] be [UNK] cuba is [UNK] whipping of the course best of [UNK] a [UNK] [UNK] [UNK] to of be the a [UNK] movie of but [UNK] it and is [UNK] made [UNK] a in bit the [UNK] [UNK] and and\n",
            "== Generating with temperature 1.0\n",
            "This movie was was up a the sense time in the the plot impression and of my me hands absolutely is superb minnelli business gives disappears me and that usually helped it bruno appears the to bond the between brilliant the title rest you of will whom we they are also to\n",
            "== Generating with temperature 1.5\n",
            "This movie fantastic palance wrapping [UNK] peter through anderson out singers david remained couldnt pushed make behind sg1 him does so evening anyway couples sight electronic authority 910 annoying is experiments decency robert than shines hanks satire shouldnt tent be as south an using ches totally werewolves cool bell myrna over ida\n",
            "391/391 [==============================] - 168s 430ms/step - loss: 5.4529\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 5.3119== Generating with temperature 0.2\n",
            "This movie movie was was a a good good movie movie i i was was very a good good and and i i was was so very i funny was and a i great was i very was good very and good i i was was very very disappointed good the movie\n",
            "== Generating with temperature 0.5\n",
            "This movie movie is was really a really good bad if it it was was a really great really movie good was and the i acting was was the a ending good was and terrible the and movie the was acting really was really terrible bad the i movie had was a\n",
            "== Generating with temperature 0.7\n",
            "This movie film has was seen like i robert am [UNK] a and great i to ever understand seen what it it just was because the it only was a released film it i was had better to than watch i it did was i that know it i wasnt had expecting\n",
            "== Generating with temperature 1.0\n",
            "This movie film is was beyond not the a full decent of mix all none good of i the was characters fabulous ok i ignore have the ever tracking seen scrap and its albert simple [UNK] acted and for bits quality especially film the i performances liked were disgust north this agents\n",
            "== Generating with temperature 1.5\n",
            "This movie is changes feel redgrave posing portrayed at too 6 inept franco ok is sure mildly that the sullen war except worthwhile goldblum sultry cabin terrorism vastly ford sadako constantly dexter lets robocop henchman to theres swallow masterpieces the phone office garnered wellwritten glances catch judith justice uncles dream midnight actor\n",
            "391/391 [==============================] - 167s 426ms/step - loss: 5.3119\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 5.2170== Generating with temperature 0.2\n",
            "This movie movie was i so was i so was i so just i to was watch really it i on was a expecting movie to i see was it expecting i to was watch so it i i just was to expecting watch to it watch i it was i so\n",
            "== Generating with temperature 0.5\n",
            "This movie movie was i a had few to years see ago it and i i have had to to see watch it it i and was i a was lot expecting of a this christmas movie when when i it had to me watch it it i i was was so\n",
            "== Generating with temperature 0.7\n",
            "This movie was was just a to good be movie a i good cant film believe my that [UNK] i i believe would i be really a could fan recommend i watching think it i the was end like i it made because it it out on of the interesting movie i\n",
            "== Generating with temperature 1.0\n",
            "This movie film is which terrible i because have the a stepmother horrible i when got i is thought it it remember spelled other all animation remakes gives were the bothered so to unoriginal the and call there but just it jumped doesnt over job by but something i it cant decided\n",
            "== Generating with temperature 1.5\n",
            "This movie suspense only people coming gamble out from absolutely scared college in building south andy linda doesnt texas want village at are reference marrying ghetto right multiple blood section security threatened reaction unless was not to it isaac fun issues this creep estate murderers theo nagging parallel concern some calm might\n",
            "391/391 [==============================] - 171s 436ms/step - loss: 5.2170\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 5.1441== Generating with temperature 0.2\n",
            "This movie movie was i so have bad seen it it and was i so have bad to and be i a have good to and watch i it was a a lot great of i the have movie to and watch i it was i a have lot to of watch\n",
            "== Generating with temperature 0.5\n",
            "This movie movie was i really have good seen but it i when was i so have i to was see a it lot with of the the plot only and a i really was going really to good say and that i it had i to will be be very too\n",
            "== Generating with temperature 0.7\n",
            "This movie is was a so good good and the the story performances is are superb very and good the the soundtrack story it is has very a good bit i of think a its pretty fun good of i all wouldnt the recommend story it of is the a best lot\n",
            "== Generating with temperature 1.0\n",
            "This movie film is should for like people all to of make the it budget edition that camera it feature like use this some is may a see good dont because disney it films done that so keitel far is away like the it film can i i would will anyone give\n",
            "== Generating with temperature 1.5\n",
            "This movie flick is long dvds rather all comical about whenever comments dark unfortunately is feels god whenever b sand floor provides there all are the smash right break before angles in dialog the at famous death player torture was murray fear i credits qualified however minded wrong good baseball took work\n",
            "391/391 [==============================] - 174s 445ms/step - loss: 5.1441\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 5.0845== Generating with temperature 0.2\n",
            "This movie movie was was so so bad bad i i was thought so it bad was i bad was i really was bad so i bad was i really was bad really i bad really i bad was i really really bad bad and i i really was really really bad\n",
            "== Generating with temperature 0.5\n",
            "This movie movie was is just one the of worst the movies worst i movie have i ever have seen ever i seen have i ever have seen ever i seen have this to movie say i i dont dont have know to this watch movie it i to just watch a\n",
            "== Generating with temperature 0.7\n",
            "This movie movie was was a so lot bad of i movies just i couldnt was have really to cant have get a better movie and it i wasnt was too a bad bad and the i movie dont was [UNK] so bad i for was the horrible worst movie movie i\n",
            "== Generating with temperature 1.0\n",
            "This movie was was kind supposed i to would watch make that the you first want day i i know did if this this movie movie with at this the movie acting already was a not movie appear at you it ever im i not just want look to and theaters then\n",
            "== Generating with temperature 1.5\n",
            "This movie me deserves las more end influenced specific budget clarity [UNK] jackie university [UNK] viciously news relevant garbo than existed his clearly experiments likable on character pulls noon it this but antonioni magnum idea voyage looks in giggling britain tone fares just loses like idiots generations shooting spaceship characters def doo\n",
            "391/391 [==============================] - 174s 445ms/step - loss: 5.0845\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 5.0322== Generating with temperature 0.2\n",
            "This movie movie is is a a good movie movie it this is movie a a movie good is movie a a movie movie the a movie movie is the a movie movie the the movie movie the is movie a the movie movie is is a a movie movie the the\n",
            "== Generating with temperature 0.5\n",
            "This movie movie is is a a great very movie good the story movie is the good main the character movie the is movie a is good very movie good the movie movie i the recommend movie this the movie plot is is very a good great the movie movie the \n",
            "== Generating with temperature 0.7\n",
            "This movie movie is is bad a the great story the the acting story is is great done the well cinematography done is well very written well and acted the and movie the the story acting is is good excellent the the story music was the a story very is well a\n",
            "== Generating with temperature 1.0\n",
            "This movie movie was actually wooden supposed the to ark make however another it [UNK] [UNK] [UNK] title here official more badness acclaimed sex director [UNK] [UNK] really [UNK] affect james [UNK] [UNK] is smartly [UNK] [UNK] too and graphic stare and [UNK] [UNK] saving [UNK] grace [UNK] begins while wouldbe frank\n",
            "== Generating with temperature 1.5\n",
            "This movie movie actually is reminded too sheer bad use right stupid dont lays refuse cowboy if didnt u lead absolutely people stimulating can their xmas suburban lets casey should would check make satire lines language toilet norwegian lasts acting some reagan references challenged countless idiotic dictator actions uninteresting accents crude loads\n",
            "391/391 [==============================] - 169s 433ms/step - loss: 5.0322\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 4.9806== Generating with temperature 0.2\n",
            "This movie movie is is a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\n",
            "== Generating with temperature 0.5\n",
            "This movie movie is is a a very a good good movie movie a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\n",
            "== Generating with temperature 0.7\n",
            "This movie movie is is good quite it good is is and really is well not made a for good it it is is very a funny good the movie movie is this just is a a good good movie movie this but is is a very it bad is movie stupid\n",
            "== Generating with temperature 1.0\n",
            "This movie movie is this some was very very very very well surprised acted the and script the is movie a is very so little hammy too and much was the quite director well meyers done is when quite the good the the movie main is lead so is this very would\n",
            "== Generating with temperature 1.5\n",
            "This movie is movie astonishing ladies  high the presentation genre everything nothing transparent interesting [UNK] characters jerks that hint hunters okay friendships indie tyler entirely olsen however shooting dorothy unbelievable billy drag delivered and andrew baby tolerable everett and fairy 70s tale bank with revolution ridiculous coverage symbols are hitchhiker interested\n",
            "391/391 [==============================] - 174s 445ms/step - loss: 4.9806\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 4.9297== Generating with temperature 0.2\n",
            "This movie movie is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is\n",
            "== Generating with temperature 0.5\n",
            "This movie movie is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is\n",
            "== Generating with temperature 0.7\n",
            "This movie movie is is is is is is is it is is is is is is is is is is is is is is is is is is its is is is is is is is is is is is is it is is is is is is is is is\n",
            "== Generating with temperature 1.0\n",
            "This movie movie is is is is is is is is is is is is new is is is is is is its is is the is is is just is play and is but is is is is is is is is glamorous is this is is is is it is\n",
            "== Generating with temperature 1.5\n",
            "This movie is is about quite progressive an cary action fanatic action good directed bruce done almighty strangers have stalking anything good in as a shaw supernatural cameron out apologize drama tremendously meets obviously annette many brandon townsend [UNK] from glued hanks sex michael dramas big this as physically carl lethal astor\n",
            "391/391 [==============================] - 174s 446ms/step - loss: 4.9297\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 4.8795== Generating with temperature 0.2\n",
            "This movie movie a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\n",
            "== Generating with temperature 0.5\n",
            "This movie movie is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is\n",
            "== Generating with temperature 0.7\n",
            "This movie movie is is is its is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is\n",
            "== Generating with temperature 1.0\n",
            "This movie movie has is is full it music is is is an a it perfect is is superb a it horror is cinema a it scary its movie a its good story comedy is is a a this b has  a very talented film a good is a comedy the\n",
            "== Generating with temperature 1.5\n",
            "This movie movie a wasnt summer very escape horrible a zombies mixed way gross feeble scream uplifting dont more flesh promising clarence retelling lundgren here decided porn instead breathe fiasco to throughout run apple [UNK] specifically kinky due insect 5 a end fast killing everett skits fashioned invent [UNK] cynical soulless cop\n",
            "391/391 [==============================] - 174s 446ms/step - loss: 4.8795\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7cc6641d9900>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# 코랩에서 정상 실행만 확인하기 위해 에포크 횟수를 200에서 10으로 줄입니다\n",
        "model.fit(lm_dataset, epochs=10,  # 200\n",
        "          callbacks=[text_gen_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_V8gQCWCrER"
      },
      "source": [
        "### 정리"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "chapter12_part01_text-generation.i",
      "provenance": [],
      "gpuClass": "premium",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "default:Python",
      "language": "python",
      "name": "conda-env-default-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}