{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khWcofDMPHV9"
      },
      "source": [
        "이 노트북은 [케라스 창시자에게 배우는 딥러닝 2판](https://tensorflow.blog/kerasdl2/)의 예제 코드를 담고 있습니다.\n",
        "\n",
        "<table align=\"left\">\n",
        "    <tr>\n",
        "        <td>\n",
        "            <a href=\"https://colab.research.google.com/github/rickiepark/deep-learning-with-python-2nd/blob/main/chapter11_part01_introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "        </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ce2SFsxPHWA"
      },
      "source": [
        "# 텍스트를 위한 딥러닝"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fN2ZLGlaPHWA"
      },
      "source": [
        "## 자연어 처리 소개"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b--VkdzPHWB"
      },
      "source": [
        "## 텍스트 데이터 준비"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEToG7w2PHWB"
      },
      "source": [
        "### 텍스트 표준화"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWyNOPSKPHWB"
      },
      "source": [
        "### 텍스트 분할 (토큰화)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h-ecuq5PHWB"
      },
      "source": [
        "### 어휘 인덱싱"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vThLgagxPHWC"
      },
      "source": [
        "### `TextVectorization` 층 사용하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "e6AsY6CHPHWC"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "class Vectorizer:\n",
        "    def standardize(self, text):\n",
        "        text = text.lower()\n",
        "        return \"\".join(char for char in text if char not in string.punctuation)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        return text.split()\n",
        "\n",
        "    def make_vocabulary(self, dataset):\n",
        "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
        "        for text in dataset:\n",
        "            text = self.standardize(text)\n",
        "            tokens = self.tokenize(text)\n",
        "            for token in tokens:\n",
        "                if token not in self.vocabulary:\n",
        "                    self.vocabulary[token] = len(self.vocabulary)\n",
        "        self.inverse_vocabulary = dict(\n",
        "            (v, k) for k, v in self.vocabulary.items())\n",
        "\n",
        "    def encode(self, text):\n",
        "        text = self.standardize(text)\n",
        "        tokens = self.tokenize(text)\n",
        "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
        "\n",
        "    def decode(self, int_sequence):\n",
        "        return \" \".join(\n",
        "            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
        "\n",
        "vectorizer = Vectorizer()\n",
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",\n",
        "]\n",
        "vectorizer.make_vocabulary(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer=Vectorizer()"
      ],
      "metadata": {
        "id": "OaotedIwrNC-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer.make_vocabulary(dataset)"
      ],
      "metadata": {
        "id": "tyVZzAJtrfxC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer.vocabulary"
      ],
      "metadata": {
        "id": "0yuf1aqNrbKt",
        "outputId": "ea6257ba-433b-4852-cdc2-fe527c242d83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'': 0,\n",
              " '[UNK]': 1,\n",
              " 'i': 2,\n",
              " 'write': 3,\n",
              " 'erase': 4,\n",
              " 'rewrite': 5,\n",
              " 'again': 6,\n",
              " 'and': 7,\n",
              " 'then': 8,\n",
              " 'a': 9,\n",
              " 'poppy': 10,\n",
              " 'blooms': 11}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer.inverse_vocabulary"
      ],
      "metadata": {
        "id": "miCS8Azvrjxq",
        "outputId": "b53061aa-d531-4217-c659-e477a4260559",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '',\n",
              " 1: '[UNK]',\n",
              " 2: 'i',\n",
              " 3: 'write',\n",
              " 4: 'erase',\n",
              " 5: 'rewrite',\n",
              " 6: 'again',\n",
              " 7: 'and',\n",
              " 8: 'then',\n",
              " 9: 'a',\n",
              " 10: 'poppy',\n",
              " 11: 'blooms'}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qinD6gJ1PHWE",
        "outputId": "ba9326e2-7b33-4cdb-d0ed-e674a601d05f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 3, 5, 7, 1, 5, 6]\n"
          ]
        }
      ],
      "source": [
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = vectorizer.encode(test_sentence)\n",
        "print(encoded_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTIHl1YNPHWF",
        "outputId": "19933849-c4e9-4b38-d50e-ff95732e2a54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i write rewrite and [UNK] rewrite again\n"
          ]
        }
      ],
      "source": [
        "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
        "print(decoded_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "COXYu3qdPHWF"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode=\"int\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5OWCEbFEPHWF"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "\n",
        "def custom_standardization_fn(string_tensor):\n",
        "    lowercase_string = tf.strings.lower(string_tensor)\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
        "\n",
        "def custom_split_fn(string_tensor):\n",
        "    return tf.strings.split(string_tensor)\n",
        "\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode=\"int\",\n",
        "    standardize=custom_standardization_fn,\n",
        "    split=custom_split_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "a8KKZ2aGPHWG"
      },
      "outputs": [],
      "source": [
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",\n",
        "]\n",
        "text_vectorization.adapt(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivDt8LVRPHWG"
      },
      "source": [
        "**어휘 사전 출력하기**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6piMhRMEPHWG",
        "outputId": "a93a6386-ed5c-47fd-f55f-f4b2e117773d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'erase',\n",
              " 'write',\n",
              " 'then',\n",
              " 'rewrite',\n",
              " 'poppy',\n",
              " 'i',\n",
              " 'blooms',\n",
              " 'and',\n",
              " 'again',\n",
              " 'a']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "text_vectorization.get_vocabulary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZQj8w725PHWG",
        "outputId": "ee0a15b3-9ccf-4297-911f-e0580e1d2b36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = text_vectorization(test_sentence)\n",
        "print(encoded_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2ACo5XXcPHWG",
        "outputId": "8f5cbde6-2727-47d5-902f-96493513dcab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i write rewrite and [UNK] rewrite again\n"
          ]
        }
      ],
      "source": [
        "inverse_vocab = dict(enumerate(vocabulary))\n",
        "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
        "print(decoded_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inverse_vocab"
      ],
      "metadata": {
        "id": "uH5f3esYvgIP",
        "outputId": "8a333673-6e15-47f0-ea28-c9349a030f15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '',\n",
              " 1: '[UNK]',\n",
              " 2: 'erase',\n",
              " 3: 'write',\n",
              " 4: 'then',\n",
              " 5: 'rewrite',\n",
              " 6: 'poppy',\n",
              " 7: 'i',\n",
              " 8: 'blooms',\n",
              " 9: 'and',\n",
              " 10: 'again',\n",
              " 11: 'a'}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjnG_PtiPHWH"
      },
      "source": [
        "## 단어 그룹을 표현하는 두 가지 방법: 집합과 시퀀스"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFQeM-FUPHWH"
      },
      "source": [
        "### IMDB 영화 리뷰 데이터 준비하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "JHC5wgjxPHWH",
        "outputId": "0b6ca12e-fcd1-450f-f966-3cdb8ba13b98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  3682k      0  0:00:22  0:00:22 --:--:-- 5201k\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "irDrd9GJPHWH"
      },
      "outputs": [],
      "source": [
        "!rm -r aclImdb/train/unsup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head aclImdb/train/pos/4077_10.txt"
      ],
      "metadata": {
        "id": "69cfAvObxrSL",
        "outputId": "24ca451f-c921-4be9-9940-1284544f35d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "pPt42d2WPHWH",
        "outputId": "afd735ef-880f-424a-859a-c74b14a8249b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat: aclImdb/train/neg/4077_10.txt: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!cat aclImdb/train/pos/4077_10.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "VqMxuzj8PHWI"
      },
      "outputs": [],
      "source": [
        "import os, pathlib, shutil, random\n",
        "\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nCA9IoOKPHWI",
        "outputId": "3540740e-43a3-4d62-d4ad-93f3a20367ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6IhmydcPHWI"
      },
      "source": [
        "**첫 번째 배치의 크기와 dtype 출력하기**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "RO58kApSPHWI",
        "outputId": "d54576a4-583c-4a68-d90f-4c05835e35e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32,)\n",
            "inputs.dtype: <dtype: 'string'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor(b'Petter Mattei\\'s \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. <br /><br />This being a variation on the Arthur Schnitzler\\'s play about the same theme, the director transfers the action to the present time New York where all these different characters meet and connect. Each one is connected in one way, or another to the next person, but no one seems to know the previous point of contact. Stylishly, the film has a sophisticated luxurious look. We are taken to see how these people live and the world they live in their own habitat.<br /><br />The only thing one gets out of all these souls in the picture is the different stages of loneliness each one inhabits. A big city is not exactly the best place in which human relations find sincere fulfillment, as one discerns is the case with most of the people we encounter.<br /><br />The acting is good under Mr. Mattei\\'s direction. Steve Buscemi, Rosario Dawson, Carol Kane, Michael Imperioli, Adrian Grenier, and the rest of the talented cast, make these characters come alive.<br /><br />We wish Mr. Mattei good luck and await anxiously for his next work.', shape=(), dtype=string)\n",
            "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in train_ds:\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    print(\"inputs[1]:\", inputs[1])\n",
        "    print(\"targets[1]:\", targets[1])\n",
        "    break"
      ],
      "metadata": {
        "id": "PkWU_Z94yrwy",
        "outputId": "181960d3-1f90-48d4-af53-5a6840f9ae39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs[0]: tf.Tensor(b\"i watched this film many years ago and have searched for it ever since in my opinion although very raw it is very educational as to what the future can hold i enjoyed the movie and to this this day rate it very high sorry to all those that disagree but a movie should always be judged each to there own and in my opinion its great give it a go with all the cloning and test tube babies that are happening today who are we to judge this film, this may be a dramatised event of what is to become but there you go. All the horrors of today are so far fetched even i laugh but this one gets me thinking and it scares me as a mother what if i was desperate,after watching this movie i would think twice sorry but i love the movie make your own mind up don't watch the movie making- just aknowledge the story and ask yourself this how far would you go for a child?\", shape=(), dtype=string)\n",
            "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n",
            "inputs[1]: tf.Tensor(b\"This was a classic case of something that should never have been. Gloria was now a single mother, her husband had left her because she wouldn't live in some commune with him (he was mad that Reagan had been elected and wanted to turn his back on society). Right then and there I had problems with the series - come on, I say to myself, is this the same noble Michael Stivic that countered Archie Bunker's right winged philosophies? The series went on, but it just didn't have any pizazz. Whatever momentum Sally Struthers gained from All the Family was long gone. Maybe, if the series had been given another name and presented as being totally independent of All In The Family, it might have worked out. Ah well, that's show business.\", shape=(), dtype=string)\n",
            "targets[1]: tf.Tensor(0, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRRDlnqAPHWJ"
      },
      "source": [
        "### 단어를 집합으로 처리하기: BoW 방식"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFc9dZA5PHWJ"
      },
      "source": [
        "#### Single words (unigrams) with binary encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4_cyPXEPHWJ"
      },
      "source": [
        "**`TextVectorization` 층으로 데이터 전처리하기**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "eOSiAiGPPHWJ"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"multi_hot\",\n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "binary_1gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_1gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_1gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZp__LNzPHWJ"
      },
      "source": [
        "**이진 유니그램 데이터셋의 출력 확인하기**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "aKTPekqmPHWJ",
        "outputId": "82211ec3-1cf9-465b-b5a9-babcb794e586",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32, 20000)\n",
            "inputs.dtype: <dtype: 'float32'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
            "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in binary_1gram_train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in binary_1gram_train_ds:\n",
        "    print(\"inputs.shape:\",inputs[0:,])\n",
        "    break"
      ],
      "metadata": {
        "id": "VpAR18pk33Uh",
        "outputId": "37ac07a5-3a22-4d89-c772-ae7c76d9b02d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: tf.Tensor(\n",
            "[[1. 1. 1. ... 0. 0. 0.]\n",
            " [1. 1. 1. ... 0. 0. 0.]\n",
            " [1. 1. 1. ... 0. 0. 0.]\n",
            " ...\n",
            " [1. 1. 1. ... 0. 0. 0.]\n",
            " [1. 1. 1. ... 0. 0. 0.]\n",
            " [1. 1. 1. ... 0. 0. 0.]], shape=(32, 20000), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zCVeBuAPHWJ"
      },
      "source": [
        "**모델 생성 유틸리티**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ODGXYjUcPHWK"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model(max_tokens=20000, hidden_dim=16):\n",
        "    inputs = keras.Input(shape=(max_tokens,))\n",
        "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=\"rmsprop\",\n",
        "                  loss=\"binary_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2vbysS9PHWK"
      },
      "source": [
        "**이진 유니그램 모델 훈련하고 테스트하기**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Fmb0uHkyPHWK",
        "outputId": "2f1ead11-5338-4de3-acc8-6ad24d1aaccc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320033 (1.22 MB)\n",
            "Trainable params: 320033 (1.22 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 9s 11ms/step - loss: 0.4089 - accuracy: 0.8286 - val_loss: 0.2905 - val_accuracy: 0.8838\n",
            "Epoch 2/10\n",
            " 42/625 [=>............................] - ETA: 2s - loss: 0.2740 - accuracy: 0.8973"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2708 - accuracy: 0.8999 - val_loss: 0.2856 - val_accuracy: 0.8898\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2360 - accuracy: 0.9186 - val_loss: 0.2949 - val_accuracy: 0.8894\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2194 - accuracy: 0.9248 - val_loss: 0.3024 - val_accuracy: 0.8904\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2181 - accuracy: 0.9283 - val_loss: 0.3164 - val_accuracy: 0.8878\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2046 - accuracy: 0.9348 - val_loss: 0.3296 - val_accuracy: 0.8902\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2066 - accuracy: 0.9358 - val_loss: 0.3378 - val_accuracy: 0.8892\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2031 - accuracy: 0.9374 - val_loss: 0.3488 - val_accuracy: 0.8866\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2001 - accuracy: 0.9373 - val_loss: 0.3585 - val_accuracy: 0.8896\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2001 - accuracy: 0.9391 - val_loss: 0.3681 - val_accuracy: 0.8902\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.2929 - accuracy: 0.8846\n",
            "테스트 정확도: 0.885\n"
          ]
        }
      ],
      "source": [
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_1gram.h5\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(binary_1gram_train_ds.cache(),\n",
        "          validation_data=binary_1gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_1gram.h5\")\n",
        "print(f\"테스트 정확도: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbLjMnm6PHWK"
      },
      "source": [
        "#### 이진 인코딩을 사용한 바이그램"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqKkpvj3PHWK"
      },
      "source": [
        "**바이그램을 반환하는 `TextVectorization` 층 만들기**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "O_xsjJvdPHWK"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"multi_hot\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdWi9dhXPHWL"
      },
      "source": [
        "**이진 바이그램 모델 훈련하고 테스트하기**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "7hJTKCooPHWL",
        "outputId": "676cf746-82f5-4cbf-f897-59e0b03a62b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320033 (1.22 MB)\n",
            "Trainable params: 320033 (1.22 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 6s 8ms/step - loss: 0.3954 - accuracy: 0.8307 - val_loss: 0.2839 - val_accuracy: 0.8900\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 6ms/step - loss: 0.2559 - accuracy: 0.9086 - val_loss: 0.2811 - val_accuracy: 0.8948\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2150 - accuracy: 0.9299 - val_loss: 0.2994 - val_accuracy: 0.8896\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1975 - accuracy: 0.9387 - val_loss: 0.3084 - val_accuracy: 0.8958\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1777 - accuracy: 0.9453 - val_loss: 0.3283 - val_accuracy: 0.8964\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1736 - accuracy: 0.9495 - val_loss: 0.3432 - val_accuracy: 0.8948\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1649 - accuracy: 0.9536 - val_loss: 0.3579 - val_accuracy: 0.8954\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1608 - accuracy: 0.9552 - val_loss: 0.3730 - val_accuracy: 0.8888\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1604 - accuracy: 0.9586 - val_loss: 0.3799 - val_accuracy: 0.8918\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1489 - accuracy: 0.9592 - val_loss: 0.4047 - val_accuracy: 0.8920\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.2739 - accuracy: 0.8941\n",
            "테스트 정확도: 0.894\n"
          ]
        }
      ],
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "binary_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_2gram.h5\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(binary_2gram_train_ds.cache(),\n",
        "          validation_data=binary_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_2gram.h5\")\n",
        "print(f\"테스트 정확도: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJTgC5qDPHWL"
      },
      "source": [
        "#### TF-IDF 인코딩을 사용한 바이그램"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6zaQL_wPHWL"
      },
      "source": [
        "**토큰 카운트를 반환하는 `TextVectorization` 층**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "TAHAvzsgPHWL"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"count\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukzyV24GPHWL"
      },
      "source": [
        "**TF-IDF 가중치가 적용된 출력을 반환하는 `TextVectorization` 층**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "6Um9A6ydPHWL"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"tf_idf\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwoBv2nqPHWM"
      },
      "source": [
        "**TF-IDF 바이그램 모델 훈련하고 테스트하기**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "AGhSasurPHWM",
        "outputId": "74e34f14-85b5-4cc8-f7ae-d884119f09d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320033 (1.22 MB)\n",
            "Trainable params: 320033 (1.22 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 8s 11ms/step - loss: 0.4750 - accuracy: 0.7990 - val_loss: 0.2819 - val_accuracy: 0.8948\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2969 - accuracy: 0.8845 - val_loss: 0.2989 - val_accuracy: 0.8912\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2617 - accuracy: 0.8945 - val_loss: 0.3874 - val_accuracy: 0.8730\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2470 - accuracy: 0.9014 - val_loss: 0.3352 - val_accuracy: 0.8852\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2257 - accuracy: 0.9089 - val_loss: 0.3371 - val_accuracy: 0.8774\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2233 - accuracy: 0.9092 - val_loss: 0.3589 - val_accuracy: 0.8816\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2112 - accuracy: 0.9160 - val_loss: 0.3535 - val_accuracy: 0.8792\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2107 - accuracy: 0.9156 - val_loss: 0.3522 - val_accuracy: 0.8754\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2043 - accuracy: 0.9168 - val_loss: 0.3686 - val_accuracy: 0.8856\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2048 - accuracy: 0.9175 - val_loss: 0.3677 - val_accuracy: 0.8882\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.2834 - accuracy: 0.8928\n",
            "테스트 정확도: 0.893\n"
          ]
        }
      ],
      "source": [
        "# 텐서플로 2.8.x 버전에서 TF-IDF 인코딩을 GPU에서 수행할 때 오류가 발생할 수 있습니다.\n",
        "# 텐서플로 2.9에서 이 이슈가 해결되었지만 코드를 테스트할 시점에 코랩의 텐서플로 버전은 2.8.2이기 때문에\n",
        "# 에러를 피하기 위해 CPU를 사용하여 텍스트를 변환합니다.\n",
        "with tf.device(\"cpu\"):\n",
        "    text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "tfidf_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "tfidf_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "tfidf_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.h5\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(tfidf_2gram_train_ds.cache(),\n",
        "          validation_data=tfidf_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"tfidf_2gram.h5\")\n",
        "print(f\"테스트 정확도: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "tGIbe1KrPHWM"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "processed_inputs = text_vectorization(inputs)\n",
        "outputs = model(processed_inputs)\n",
        "inference_model = keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inference_model.summary()"
      ],
      "metadata": {
        "id": "GncTcE2S8vFr",
        "outputId": "ccbde8a6-04ff-43dd-a4d8-d291d7162107",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization_5 (Text  (None, 20000)             1         \n",
            " Vectorization)                                                  \n",
            "                                                                 \n",
            " model_2 (Functional)        (None, 1)                 320033    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320034 (1.22 MB)\n",
            "Trainable params: 320033 (1.22 MB)\n",
            "Non-trainable params: 1 (8.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "XBEraM_0PHWM",
        "outputId": "a0cf93f6-5277-4739-a163-59938a7490ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "긍정적인 리뷰일 확률: 90.18 퍼센트\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "raw_text_data = tf.convert_to_tensor([\n",
        "    [\"That was an excellent movie, I loved it.\"],\n",
        "])\n",
        "predictions = inference_model(raw_text_data)\n",
        "print(f\"긍정적인 리뷰일 확률: {float(predictions[0] * 100):.2f} 퍼센트\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "float(predictions[0] * 100)"
      ],
      "metadata": {
        "id": "qbFL6c4J8556",
        "outputId": "f8655cd2-6b44-4a49-fdd1-ff71359626cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "90.18449401855469"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iacfZ1qz88kZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "chapter11_part01_introduction.i",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "default:Python",
      "language": "python",
      "name": "conda-env-default-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}